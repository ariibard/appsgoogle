#   group_by(category) %>%
#   summarise(descargas = sum(installs)) %>%
#   arrange(desc(descargas))
ggplotly(g)
rm(g)
g4 <- ggplot(df_final, aes(x=type, y=installs, fill=type)) +
geom_boxplot() +
scale_fill_brewer(palette = "Set2") +
scale_y_continuous(labels = scales::comma) +
theme_classic() +
ggtitle(label = "Boxplot instalaciones por tipo") +
guides(fill=FALSE)
ggplotly(g4)
rm(g4)
# Convertir a gráfico interactivo
p <- ggplot(df_final, aes(x = reorder(grupo_edades,installs), y = installs)) +
geom_bar(stat = "identity", fill = "#FB8072") +
labs(title = "Cantidad de instalaciones por grupos de edades",
x = "Grupos de Edades", y = "Cantidad de instalaciones") +
theme_classic()
ggplotly(p)
rm(p)
p <- df_final %>%
mutate(rating_imp = round(rating_imp,0)) %>%
ggplot(aes(x = reorder(rating_imp,installs), y = installs)) +
geom_bar(stat = "identity", fill = "#FB8072") +
labs(title = "Cantidad de instalaciones por rating",
x = "Rating", y = "Cantidad de instalaciones") +
theme_classic()
ggplotly(p)
rm(p)
ggplot(df_final, aes(x = installs, y = size_num_imp, fill = size_num_imp)) +
geom_boxplot(fill = "#FB8072") + theme_classic()  +
scale_y_continuous(labels = scales::comma) +
ylab("Instalaciones") +
xlab("Tamaño") +
ggtitle("Relación entre tamaño y cantidad de instalaciones")
esquisse::esquisser(df_final)
ggplot(df_final, aes(x = installs, y = reviews, fill = reviews)) +
geom_boxplot(fill = "#FB8072") + theme_classic()  +
scale_y_continuous(labels = scales::comma) +
ylab("Instalaciones") +
xlab("Tamaño") +
ggtitle("Relación entre tamaño y cantidad de instalaciones")
plot1 <- df_final %>%
group_by(last_updated) %>%
summarise(total_installs = sum(installs)) %>%
ggplot(aes(x = last_updated, y = total_installs)) +
geom_line(color = "#FF689f", size = 1) +
labs(x = "Fecha de actualización", y = "Instalaciones",
title = "Total de instalaciones por fecha de actualización") +
theme_classic() +
theme(plot.title = element_text(size = 14, face = "bold"),
axis.text.x = element_text(angle = 90, vjust = 0.5, size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 12, face = "bold"),
legend.title = element_blank(),
legend.position = "none")
ggplotly(plot1)
df_final <- df_final %>%
mutate(ano_act = year(last_updated))
#observamos la distribución por año
df_final %>%
group_by(ano_act) %>%
summarise(N=n()) %>%
gt()
#Guardamos el CSV
write.csv(df_final, "df_final.csv")
#armamos una matriz de correlación
matriz_df <- df_final %>%
select_if(is.numeric)
matriz_df <- cor(matriz_df)
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "lower",  lab=TRUE, hc.order = TRUE, title = "Matriz de correlación R", colors = c("#6D9EC1", "white", "purple")) +
theme(text = element_text(size = 10),
axis.text.x =  element_text(angle=90, hjust=1, size = 7),
axis.text.y =  element_text(size = 7))
ggplotly(c1)
#Borramos los datasets para optimizar memoria
rm(c1)
rm(matriz_df)
#armamos una matriz de correlación
matriz_df <- df_final %>%
select_if(is.numeric)
matriz_df <- cor(matriz_df)
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "lower",  lab=TRUE, hc.order = TRUE, title = "Matriz de correlación R", colors = c("#6D9EC1", "white", "purple")) +
theme(text = element_text(size = 10),
axis.text.x =  element_text(angle=90, hjust=1, size = 7),
axis.text.y =  element_text(size = 7))
ggplotly(c1)
matriz_df
#convertimos a factor las variables categóricas
#Borramos los datasets para optimizar memoria
rm(plot1)
rm(count_outliers)
rm(fa_apps)
### Distribución de la variable installs
df_final %>%
group_by(installs) %>%
summarise(N=n()) %>%
gt()
#Creo variables dummys y estandarizo las variables
df_accesin <- df_final %>%
mutate(category = as.factor(category),
grupo_edades = as.factor(grupo_edades),
ano_act = as.factor(ano_act),
genre= as.factor(genre),
popular = ifelse(installs >= 100000, 1,0)) %>%
select(-last_updated, -type, -genre)%>%
recipes::step_scale(all_numeric(), except = c("type_bin","popular", "installs")) %>%
select(-steps)
View(df_final)
#convertimos a factor las variables categóricas
#Borramos los datasets para optimizar memoria
rm(plot1)
rm(count_outliers)
rm(fa_apps)
### Distribución de la variable installs
df_final %>%
group_by(installs) %>%
summarise(N=n()) %>%
gt()
#Creo variables dummys y estandarizo las variables
df_accesin <- df_final %>%
mutate(category = as.factor(category),
grupo_edades = as.factor(grupo_edades),
ano_act = as.factor(ano_act),
popular = ifelse(installs >= 100000, 1,0)) %>%
select(-last_updated, -type, -genres)%>%
recipes::step_scale(all_numeric(), except = c("type_bin","popular", "installs")) %>%
select(-steps)
df_accesin %>% group_by(android_ver_imp) %>% summarise(N=n())
#ponemos el nombre de la app como nombre de la fila
df_accesin$app <- paste0(seq_len(nrow(df_accesin)), "_", df_accesin$app)
rownames(df_accesin) <- df_accesin$app
df_accesin_cat <- df_accesin %>%
select_if(is.factor)
#Creating Dummy Variables
dummy<- data.frame(sapply(df_accesin_cat,function(x) data.frame(model.matrix(~x-1,data =df_accesin_cat))[,-1]))
dummy %>%
head(10) %>%
gt()
df_accesin_int <- df_accesin %>%
select_if(is.numeric)
df_analisis <- cbind(df_accesin_int,dummy)
##Creamos un dataset con la variable popular y otro con la variable installs
df_popular <- df_analisis %>%
select(-installs)
accesin_final <- df_analisis %>%
select(-popular)
#Borramos los datasets para optimizar memoria
rm(df_accesin_int)
rm(df_accesin_cat)
rm(dummy)
#convertimos a factor las variables categóricas
#Borramos los datasets para optimizar memoria
rm(plot1)
rm(count_outliers)
rm(fa_apps)
### Distribución de la variable installs
df_final %>%
group_by(installs) %>%
summarise(N=n()) %>%
gt() %>%
tab_header(title = "Apps por cantidad de instalaciones")
#Creo variables dummys y estandarizo las variables
df_accesin <- df_final %>%
mutate(category = as.factor(category),
grupo_edades = as.factor(grupo_edades),
ano_act = as.factor(ano_act),
popular = ifelse(installs >= 100000, 1,0)) %>%
select(-last_updated, -type, -genres)%>%
recipes::step_scale(all_numeric(), except = c("type_bin","popular", "installs")) %>%
select(-steps)
#df_accesin %>% group_by(android_ver_imp) %>% summarise(N=n())
#ponemos el nombre de la app como nombre de la fila
df_accesin$app <- paste0(seq_len(nrow(df_accesin)), "_", df_accesin$app)
rownames(df_accesin) <- df_accesin$app
df_accesin_cat <- df_accesin %>%
select_if(is.factor)
#Creating Dummy Variables
dummy<- data.frame(sapply(df_accesin_cat,function(x) data.frame(model.matrix(~x-1,data =df_accesin_cat))[,-1]))
dummy %>%
head(10) %>%
gt()
df_accesin_int <- df_accesin %>%
select_if(is.numeric)
df_analisis <- cbind(df_accesin_int,dummy)
##Creamos un dataset con la variable popular y otro con la variable installs
df_popular <- df_analisis %>%
select(-installs)
accesin_final <- df_analisis %>%
select(-popular)
#Borramos los datasets para optimizar memoria
rm(df_accesin_int)
rm(df_accesin_cat)
rm(dummy)
#Creamos el dataset de train y test
#fijamos una semilla
set.seed(583)
n <- nrow(accesin_final)
train_idx <- sample(1:n, n*0.7, replace = FALSE) # 70% para entrenamiento
train <- accesin_final[train_idx, ]
test <- accesin_final[-train_idx, ]
n_train = nrow(train)
train %>%
group_by(installs) %>%
summarise(Prop = round(n()/n_train,3)) %>%
gt() %>%
tab_options(page.width = "100") %>%
tab_header(title = "Proporción de Instalaciones")
#creamos el modelo
model <- rpart(installs ~., data = train, method = 'class')
#Graficamos
rpart.plot(model, main = "Árbol de clasificación", extra = 101, under = TRUE, branch.lty = 1, shadow.col = "gray")
#resumen del modelo creado
summary(model)
# Aplicamos el modelo a los datos de prueba
predict_test <- predict(model, test, type = "class")
# Creamos una tabla de contingencia para evaluar la precisión
table_mat <- table(test$installs, predict_test)
table_mat
# Calculamos la precisión del modelo
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
print(paste("Accuracy:", accuracy_Test))
accuracy_train <- function(fit) {
predict_unseen_train <- predict(fit, train, type = 'class')
table_mat <- table(train$installs, predict_unseen_train)
accuracy_train <- sum(diag(table_mat)) / sum(table_mat)
accuracy_train
}
accuracy_test <- function(fit) {
predict_unseen <- predict(fit, test, type = 'class')
table_mat <- table(test$installs, predict_unseen)
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
}
print(paste('Accuracy para train', accuracy_train(model)))
print(paste('Accuracy para test', accuracy_test(model)))
# Aplicamos el modelo a los datos de prueba
predict_test <- predict(model, test, type = "class")
# Creamos una tabla de contingencia para evaluar la precisión
table_mat <- table(test$installs, predict_test)
table_mat
# Calculamos la precisión del modelo
# accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
# print(paste("Accuracy:", accuracy_Test))
accuracy_train <- function(fit) {
predict_unseen_train <- predict(fit, train, type = 'class')
table_mat <- table(train$installs, predict_unseen_train)
accuracy_train <- sum(diag(table_mat)) / sum(table_mat)
accuracy_train
}
accuracy_test <- function(fit) {
predict_unseen <- predict(fit, test, type = 'class')
table_mat <- table(test$installs, predict_unseen)
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
}
print(paste('Accuracy para train', accuracy_train(model)))
print(paste('Accuracy para test', accuracy_test(model)))
# Obtener las predicciones del modelo en el conjunto de prueba
pred_test <- predict(model, newdata = test, type = "class")
# Matriz de confusión
confusionMatrix(table(pred_test, test$installs))
# Tabla de contingencia
table_test <- table(test$installs, pred_test)
# Precisión por instalaciones
precision <- diag(table_test) / colSums(table_test)
# Recall por instalaciones
recall <- diag(table_test) / rowSums(table_test)
# Graficar precision y recall en un gráfico de barras
barplot(precision, ylim = c(0, 1), main = "Precisión por instalaciones", xlab = "cantidad", ylab = "Precisión")
barplot(recall, ylim = c(0, 1), main = "Recall por instalaciones", xlab = "cantidad", ylab = "Recall")
# Define the range of values for the cp parameter
cp_values <- seq(0.001, 0.1, by = 0.001)
# Create the tuning grid
tune_grid <- data.frame(cp = cp_values)
# Fit the model with cross-validation and the tuning grid
fit <- train(
installs ~ .,
data = train,
method = "rpart",
tuneGrid = tune_grid,
trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE)
)
model_2 <- rpart(installs ~ ., data = train, method = "class", control = rpart.control(cp = fit$bestTune$cp))
# Plot the decision tree
rpart.plot(model_2, extra = 1,main = "Arbol" )
summary(model_2)
# Obtener las predicciones del modelo en el conjunto de prueba
pred_test <- predict(model_2, newdata = test, type = "class")
# Matriz de confusión
confusionMatrix(table(pred_test, test$installs))
predict_test <- predict(model_2, test, type = "class")
# Creamos una tabla de contingencia para evaluar la precisión
table_mat <- table(test$installs, predict_test)
table_mat
print(paste('Accuracy para train', accuracy_train(model_2)))
print(paste('Accuracy para test', accuracy_test(model_2)))
matriz_df <- accesin_final %>%
select_if(is.numeric)
matriz_df <- cor(matriz_df)
matriz_df
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "lower",  lab=TRUE, hc.order = TRUE, title = "Matriz de correlación R", colors = c("#6D9EC1", "white", "purple")) +
theme(text = element_text(size = 10),
axis.text.x =  element_text(angle=90, hjust=1, size = 7),
axis.text.y =  element_text(size = 7))
ggplotly(c1)
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "lower",  lab=FALSE, hc.order = TRUE, title = "Matriz de correlación R", colors = c("#6D9EC1", "white", "purple")) +
theme(text = element_text(size = 10),
axis.text.x =  element_text(angle=90, hjust=1, size = 7),
axis.text.y =  element_text(size = 7))
ggplotly(c1)
matriz_df <- accesin_final %>%
select_if(is.numeric)
matriz_df <- cor(matriz_df)
matriz_df
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "lower",  lab=FALSE, hc.order = TRUE, colors = c("#6D9EC1", "black", "purple")) +
ggtitle("Matriz de correlación R") +
theme(text = element_text(size = 7),
axis.text.x =  element_text(angle=90, hjust=1, size = 6))
ggplotly(c1)
matriz_df <- accesin_final %>%
select_if(is.numeric)
matriz_df <- cor(matriz_df)
matriz_df
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "lower",  lab=FALSE, hc.order = TRUE, colors = c("#6D9EC1", "white", "purple")) +
ggtitle("Matriz de correlación R") +
theme(text = element_text(size = 7),
axis.text.x =  element_text(angle=90, hjust=1, size = 2))
ggplotly(c1)
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "lower",  lab=FALSE, hc.order = TRUE, colors = c("#6D9EC1", "white", "purple")) +
ggtitle("Matriz de correlación R") +
theme(text = element_text(size = 3),
axis.text.x =  element_text(angle=90, hjust=1, size = 3),
axis.text.y =  element_text(size = 3))
ggplotly(c1)
?? ggcorrplot
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "upper",  lab=FALSE, hc.order = FALSE, colors = c("#6D9EC1", "white", "purple")) +
ggtitle("Matriz de correlación R") +
theme(text = element_text(size = 3),
axis.text.x =  element_text(angle=90, hjust=1, size = 4),
axis.text.y =  element_text(size = 4))
ggplotly(c1)
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "full",  lab=FALSE, hc.order = FALSE, colors = c("#6D9EC1", "white", "purple")) +
ggtitle("Matriz de correlación R") +
theme(text = element_text(size = 3),
axis.text.x =  element_text(angle=90, hjust=1, size = 4),
axis.text.y =  element_text(size = 4))
ggplotly(c1)
c1 <- matriz_df %>%
ggcorrplot:::ggcorrplot(type = "full",  lab=FALSE, hc.order = FALSE, colors = c("#6D9EC1", "white", "purple"), ggtheme = ggplot2::theme_classic) +
ggtitle("Matriz de correlación R") +
theme(text = element_text(size = 3),
axis.text.x =  element_text(angle=90, hjust=1, size = 4),
axis.text.y =  element_text(size = 4))
ggplotly(c1)
matriz_df
View(matriz_df)
# Seleccionar las columnas que tienen una correlación mayor a 0.5
cols_sel <- matriz_df %>%
abs() %>%
as.data.frame() %>%
rownames_to_column(var = "var1") %>%
pivot_longer(cols = -var1, names_to = "var2", values_to = "cor") %>%
filter(var1 != var2 & cor > 0.5) %>%
select(var1, var2) %>%
unlist()
# Seleccionar solo las columnas que tienen una correlación mayor a 0.5
df_sel <- df[,cols_sel]
cols_sel
cols_sel <- which(apply(matriz_df, 2, function(x) any(abs(x) > 0.5)))
df_sel <- matriz_df[, cols_sel]
View(df_sel)
# Seleccionar solo las columnas que tienen una correlación mayor a 0.5
df_sel <- df[,cols_sel]
matriz_df %>%
abs() %>%
as.data.frame() %>%
rownames_to_column(var = "var1") %>%
pivot_longer(cols = -var1, names_to = "var2", values_to = "cor")
%>%
matriz_df %>%
abs() %>%
as.data.frame() %>%
rownames_to_column(var = "var1") %>%
pivot_longer(cols = -var1, names_to = "var2", values_to = "cor") %>%
filter(var1 != var2 & cor > 0.5)
# Seleccionar las columnas que tienen una correlación mayor a 0.5
cols_sel <- matriz_df %>%
abs() %>%
as.data.frame() %>%
rownames_to_column(var = "var1") %>%
pivot_longer(cols = -var1, names_to = "var2", values_to = "cor") %>%
filter(var1 != var2 & cor > 0.3)
matriz_df %>%
abs() %>%
as.data.frame() %>%
rownames_to_column(var = "var1") %>%
pivot_longer(cols = -var1, names_to = "var2", values_to = "cor") %>%
filter(var1 != var2 & cor > 0.3)
cols_sel <- matriz_df %>%
abs() %>%
as.data.frame() %>%
rownames_to_column(var = "var1") %>%
pivot_longer(cols = -var1, names_to = "var2", values_to = "cor") %>%
filter(var1 != var2 & cor > 0.3) %>%
arrange(desc(cor))
col_sel %>%
gt() %>%
tab_header(title = "Variables con mayor correlación")
cols_sel <- matriz_df %>%
abs() %>%
as.data.frame() %>%
rownames_to_column(var = "var1") %>%
pivot_longer(cols = -var1, names_to = "var2", values_to = "cor") %>%
filter(var1 != var2 & cor > 0.3) %>%
arrange(desc(cor))
cols_sel %>%
gt() %>%
tab_header(title = "Variables con mayor correlación")
#Creamos el dataset de train y test
#fijamos una semilla
set.seed(583)
n <- nrow(accesin_final)
train_idx <- sample(1:n, n*0.7, replace = FALSE) # 70% para entrenamiento
train <- accesin_final[train_idx, ]
test <- accesin_final[-train_idx, ]
n_train = nrow(train)
#visualizamos la distribución
train %>%
group_by(installs) %>%
summarise(Prop = round(n()/n_train,3)) %>%
gt() %>%
tab_options(page.width = "100") %>%
tab_header(title = "Proporción de Instalaciones")
#creamos el modelo
model <- rpart(installs ~., data = train, method = 'class')
#Graficamos
rpart.plot(model, main = "Árbol de clasificación", extra = 101, under = TRUE, branch.lty = 1, shadow.col = "gray")
#resumen del modelo creado
summary(model)
# Aplicamos el modelo a los datos de prueba
predict_test <- predict(model, test, type = "class")
# Creamos una tabla de contingencia para evaluar la precisión
table_mat <- table(test$installs, predict_test)
table_mat
# Calculamos la precisión del modelo
# accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
# print(paste("Accuracy:", accuracy_Test))
accuracy_train <- function(fit) {
predict_unseen_train <- predict(fit, train, type = 'class')
table_mat <- table(train$installs, predict_unseen_train)
accuracy_train <- sum(diag(table_mat)) / sum(table_mat)
accuracy_train
}
accuracy_test <- function(fit) {
predict_unseen <- predict(fit, test, type = 'class')
table_mat <- table(test$installs, predict_unseen)
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
}
print(paste('Accuracy para train', accuracy_train(model)))
print(paste('Accuracy para test', accuracy_test(model)))
# Obtener las predicciones del modelo en el conjunto de prueba
pred_test <- predict(model, newdata = test, type = "class")
# Matriz de confusión
confusionMatrix(table(pred_test, test$installs))
# Tabla de contingencia
table_test <- table(test$installs, pred_test)
# Precisión por instalaciones
precision <- diag(table_test) / colSums(table_test)
# Recall por instalaciones
recall <- diag(table_test) / rowSums(table_test)
# Graficar precision y recall en un gráfico de barras
barplot(precision, ylim = c(0, 1), main = "Precisión por instalaciones", xlab = "cantidad", ylab = "Precisión")
barplot(recall, ylim = c(0, 1), main = "Recall por instalaciones", xlab = "cantidad", ylab = "Recall")
# Define the range of values for the cp parameter
cp_values <- seq(0.001, 0.1, by = 0.001)
# Create the tuning grid
tune_grid <- data.frame(cp = cp_values)
# Fit the model with cross-validation and the tuning grid
fit <- train(
installs ~ .,
data = train,
method = "rpart",
tuneGrid = tune_grid,
trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE)
)
model_2 <- rpart(installs ~ ., data = train, method = "class", control = rpart.control(cp = fit$bestTune$cp))
# Plot the decision tree
rpart.plot(model_2, extra = 1,main = "Arbol" )
summary(model_2)
# Obtener las predicciones del modelo en el conjunto de prueba
pred_test <- predict(model_2, newdata = test, type = "class")
# Matriz de confusión
confusionMatrix(table(pred_test, test$installs))
predict_test <- predict(model_2, test, type = "class")
# Creamos una tabla de contingencia para evaluar la precisión
table_mat <- table(test$installs, predict_test)
table_mat
print(paste('Accuracy para train', accuracy_train(model_2)))
print(paste('Accuracy para test', accuracy_test(model_2)))
